# Disclaimer:
This was an old exercise. Therefore it might be possible that current approaches largely differ from this approach. Also keep in mind that this task was solved under potential restrictions (solution approaches, model usage, hardware, time).
# Transformer without Attention
In this project we focuse on the question if attention is really necessary for a well performing Transformer. Our work is based on the proposed [MLP-Mixer](https://arxiv.org/abs/2105.01601). We also created a project report for this project, which can be provided upon request.